diff --git a/Makefile b/Makefile
index baa2f7d..b909519 100644
--- a/Makefile
+++ b/Makefile
@@ -3,7 +3,7 @@ CC 	= nvcc
 LD	= nvcc
 
 INC = -I/usr/local/cuda/samples/common/inc/
-NVCCFLAGS 	= -arch=sm_20
+NVCCFLAGS 	= -arch=sm_20 -g -G
 LDFLAGS	=
 LIBS 	=
 
diff --git a/src/main.cu b/src/main.cu
index 3e2ebb7..b5154dc 100644
--- a/src/main.cu
+++ b/src/main.cu
@@ -14,7 +14,38 @@ using namespace std;
 
 #define N 1e8
 
-typedef double myType;
+typedef long long myType;
+
+template <typename T>
+size_t getMaxAvailGpuMem()
+{
+
+	const size_t Mb = 1<<20; // Assuming a 1Mb page size here
+
+	size_t available, total;
+	cudaMemGetInfo(&available, &total);
+
+	T *buf_d = 0; 
+	size_t nwords = available / sizeof(T);
+	size_t words_per_Mb = Mb / sizeof(T);
+
+cout << " AVAIL = " << nwords << endl;
+cout << " PERWO = " << words_per_Mb << endl;
+
+	while(cudaMalloc((void**)&buf_d,  nwords * sizeof(T)) == cudaErrorMemoryAllocation)
+	{
+		nwords -= words_per_Mb;
+		if( nwords  < words_per_Mb)
+		{
+			// signal no free memory
+			break;
+		}
+	}
+
+	cudaDeviceReset();
+	return nwords;
+
+}
 
 // gpu wrapper function
 template <class T, class Compare>
@@ -28,34 +59,69 @@ void filterGPU_wrap(T h_data[], size_t dataSize, const T val, char h_bitvec[], C
     
     checkCudaErrors(cudaGetDeviceProperties(&deviceProp, devID));
 
-    cout << "Total global memory : " << deviceProp.totalGlobalMem << endl;
+    long long globalmem = deviceProp.totalGlobalMem;
+
+    cout << "Total global memory : " << globalmem << endl;
     
+    // Kernel configuration, where a one-dimensional
+    // grid and one-dimensional blocks are configured.
+    dim3 dimGrid(NBLOCKS);
+    dim3 dimBlock(NTHREADS);
+
+size_t available, total;
+cudaMemGetInfo(&available, &total);
+
+
+//cout << "--=------> Max avail mem = " << getMaxAvailGpuMem<myType>() << endl;
+
+//    long long maxNumElem = globalmem / sizeof(myType);
+    long long maxNumElem = available / sizeof(myType) /2;
+    long long numiter = ( dataSize + (maxNumElem -1) ) / maxNumElem;
+
+/*
+cout << "available " << available / sizeof(myType) << endl;
+cout << "total " << total / sizeof(myType) << endl;
+cout << "maxNumElem " << maxNumElem << endl;
+cout << "dataSize " << dataSize << endl;
+cout << "allocated " << MIN(maxNumElem, dataSize) << endl;
+*/
+
+
     // allocate device memory
     myType *d_data;
-    checkCudaErrors(cudaMalloc((void **) &d_data, dataSize * sizeof(myType) ));
-    // copy host memory to device
-    checkCudaErrors(cudaMemcpy(d_data, h_data, dataSize * sizeof(myType), cudaMemcpyHostToDevice));
-
+    checkCudaErrors(cudaMalloc((void **) &d_data, MIN(maxNumElem, dataSize) * sizeof(myType) ));
 
     // allocate device memory for result
     char *d_bitvec;
-    checkCudaErrors(cudaMalloc((void **) &d_bitvec, dataSize * sizeof(char)));
+    checkCudaErrors(cudaMalloc((void **) &d_bitvec, MIN(maxNumElem, dataSize) * sizeof(char)));
 
-    // Kernel configuration, where a one-dimensional
-    // grid and one-dimensional blocks are configured.
-    dim3 dimGrid(NBLOCKS);
-    dim3 dimBlock(NTHREADS);
+    long long dataRem = dataSize;
+    for(int i=0; i<numiter; i++)
+    {
+	    long long offset = i * maxNumElem;
+	    long long currDataSize = MIN(dataRem, maxNumElem);
 
-    // execute the kernel
-    filterGPU<<< dimGrid, dimBlock >>>(d_data, dataSize, val, d_bitvec, comp);
+	    cout << "offset = " << offset << "   max num elem = "<< maxNumElem << " data rem = " << dataRem << "  dsize = " << dataSize << endl;
 
-    cudaThreadSynchronize();
+	    // copy host memory to device
+	    checkCudaErrors(cudaMemcpy(d_data, (h_data + offset), currDataSize * sizeof(myType), cudaMemcpyHostToDevice));
 
-    // check if kernel execution generated and error
-    getLastCudaError("Kernel execution failed");
+	    // execute the kernel
+	    filterGPU<<< dimGrid, dimBlock >>>(d_data, currDataSize, val, d_bitvec, comp);
 
-    // copy result from device to host
-    checkCudaErrors(cudaMemcpy(h_bitvec, d_bitvec, dataSize * sizeof(char), cudaMemcpyDeviceToHost));
+	    cudaDeviceSynchronize();
+
+	    // check if kernel execution generated and error
+	    getLastCudaError("Kernel execution failed");
+
+	    // copy result from device to host
+	    checkCudaErrors(cudaMemcpy((h_bitvec + offset), d_bitvec, currDataSize * sizeof(char), cudaMemcpyDeviceToHost));
+
+	    dataRem = dataRem - currDataSize;
+
+    }
+
+//cout << "---------> DONE" << endl;
 
     // free device memory
     cudaFree(d_data);
@@ -65,6 +131,9 @@ void filterGPU_wrap(T h_data[], size_t dataSize, const T val, char h_bitvec[], C
 
 }
 
+
+
+
 template <typename T>
 void data_generate(T* datavec, size_t dataSize)
 {
